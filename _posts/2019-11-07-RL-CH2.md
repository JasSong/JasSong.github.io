---
title: "파이썬과 케라스로 배우는 강화학습 2강"
date: 2019-11-07 00:00:00 -0400
categories: 
---

Summary

- MDP(Markov Decision Process)에 대한 수식
  - 상태(State)
  - 행동(Action)
  - 보상 함수(Reward function)
  - 상태변환확률(State Transition Probability)
  - 감가율 (Discount Factor)
- 정책(Policy)
- 가치함수(Value Function)
  - 상태 가치함수 (State Value function)
  - 행동 가치함수 (Action Value function) - 큐함수
- 벨만 방정식
  - 벨만 기대방정식
  - 벨만 최적방정식



1. **MDP(Markov Decision Process)**

   순차적 행동 결정 문제를 풀 때 수학적으로 표현하는 방식.

   - 상태 (State) 

     에이전트가 관찰 가능한 상태의 **집합 S(Random Variable)** , state는 action에 따라( Transition Probability)  다음 state로 변함.

     ex) $S_{t} = s$  : 시점 t에서 가능한 상태의 집합에서 특정한 상태 s

   - 행동 (Action)

     $S_{t}$에 할 수 있는 모든 행동의 **집합 A(Random Variable)**

     ex)  $A_{t}=a$: 시점 t에서 가능한 상태의 집합에서 특정한 행동 a

   - 상태변환확률(State Transition Probability) - 환경의 모델

     t 시점 s 상태에서, t시점 s 상태에서 a 행동을 취했을때, **t+1 시점에서 s' 상태**일 확률. 
     
    $$
    P^{a}_{ss'} = P[S_{t+1}=s'|S_{t}=s, A_{t}=a]
    $$

   - 감가율(Discount Factor)

  가까운 보상에 더 큰 가중치를 두기 위한 비율 $\gamma \in {[0,1]}$

  ex) t시점에서 시간 2이 지났을때 $\gamma ^{2-1}$ 만큼 보상(Reward)에 곱하겠음.

   - 보상 함수(Reward Function)

  에이전트가 학습할 수 있는 유일한 정보. 환경으로 부터 시간 1(time step)이 지난 다음 받는 보상에 대한 기대값.

  t 시점 s 상태에서, t 시점 a 행동을 행하였을 때, t+1시점의 보상의 기대값(Expectation). 결국 행동으로 인해 보상을 받는 것이다(결과 때문에 받는 것이 아님)
  $$
  R^{s}_{a} = E[R_{t+1}|S_{t}=s, A_{t}=a]
  $$

   


2. **정책(Policy)**

   에이전트가 어떤 s(상태)에 도착하였을 어떤 a(행동)를 행해야할지 골라주는 것. 강화학습은 결국 **최적의 정책**을 목적으로 하고, MDP에서는 얻은 보상들의 합이(Reward)을 최대화하는 정책을 찾는다.
   $$
   \pi(a|s) = P[A_{t}=a|S_{t}=s]
   $$

3. **가치함수(Value Function)**


   어떠한 상태에 있으면 앞으로 얼마의 보상을 받을 것인지에 대한 기대값을 나타내는 함수

   * 상태 가치 함수(State-value function) - v(s)

     t시점 s 상태일 때, t시점 확률변수 G(Return)에 대한 기대값. 쉽게 말해 s 상태의 가치이고, 에이전트는 다음으로 갈수 있는 상태들(집합 S에 있는 것들) 중 높은 가치를 선택하는 것.  
     $$
     v(s) = E[G_{t}|S_{t}=s]
     $$

     * $G_{t}$ (Return) : t시점으로부터 감가된 보상들의 총합 
       
     $$
       G_{t} =  R_{t+1}+\gamma R_{t+2}+ \gamma ^{2}R_{t+3}+.... = \sum_{k=0}^\infty \gamma ^kR_{t+k+1}
     $$


     $$
     G_{t} = R_{t+1}+\gamma G_{t+1}
     $$
    
     t시점 G는 t+1시점의 보상 R 과 감가율읠 적용한 t+1	시점의 G의 합이됨.
    
     

   * 행동 가치 함수(Action-value function) -  큐함수

     어떤 s상태에서 어떤 a행동을 할 경우 받을 G(return)값에 대한 기대값을 나타내는 함수. 어떤 행동을 하면 얼마나 좋을까를 나타내줌. 
     $$
     q_{\pi}(s,a) = E_{\pi}[G_{t}|S_{t} = s, A_{t}=a]
     $$

     $$
     q_{\pi}(s,a) = E_{\pi}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_{t} = s, A_{t}=a]
     $$


4. **벨만방정식(Bellman Equation)**

   특정 정책에서 현재(t 시점) 상태의 가치 함수와 다음 상태(t+1시점)의 보상과 다음 상태(t+1시점) 가치함수의 합을 식을 나타낸 것

   *  벨만 기대 방정식

     상태 가치 함수
     
     $$
     v_{\pi}(s) =E_{\pi}[R_{t+1}+ \gamma v_{\pi}(S_{t+1}) | S_{t}=s]
     $$

     
     $$
     v_{\pi}(s) = \sum_{a \in A} \pi(a|s)(R_{s}^a + \gamma \sum_{s' \in S} P_{ss'}^{a} v_{\pi}(s')) -계산 가능한 벨만 방정식
     $$

     

     행동 가치 함수(큐함수)
     
     $$
     q_{\pi}(s,a) =E_{\pi}[R_{t+1}+ \gamma q_{\pi}(S_{t+1},A_{t+1})  |  S_{t}=s, A_{t}=a]
     $$

     $$
     q_{\pi}(s,a) = \sum_{a \in A} \pi(a|s)(R_{s}^a + \gamma  \sum_{s' \in S} P_{ss'}^{a} \sum_{a' \in A} \pi(a'|s') q_{\pi}(s',a')) -계산 가능한 벨만 방정식
     $$

     여기서 s',  a' 은 다음 시점의 상태와 행동을 의미함.
     
     정책 $\pi (a|s)$ 는 어떠한 행동을 할 확률을 의미함.
     
     
   
   * 벨만 최적 방정식
   
     최적 상태 가치 함수
     $$
     v_{*}(s) =\max_{\pi} v_{\pi}(s)
     $$
     최적 행동 가치 함수(큐함수)
     $$
     q_{*}(s,a) =\max_{\pi} q_{\pi}(s,a)
     $$
     






















